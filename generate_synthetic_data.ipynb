{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from statsmodels import robust\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# run pca and visualize\n",
    "\n",
    "import scipy\n",
    "\n",
    "colors = [\n",
    "'black','blue','red','green','cyan','magenta','brown','pink',\n",
    "'orange','firebrick','lawngreen','dodgerblue','crimson','orchid','slateblue',\n",
    "'darkgreen','darkorange','indianred','darkviolet','deepskyblue','greenyellow',\n",
    "'peru','cadetblue','forestgreen','slategrey','lightsteelblue','rebeccapurple',\n",
    "'darkmagenta','yellow','hotpink']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize recomputed templates over time;\n",
    "def binary_reader_waveforms(filename, n_channels, n_times, spikes, channels=None, data_type='float32'):\n",
    "    ''' Reader for loading raw binaries\n",
    "    \n",
    "        standardized_filename:  name of file contianing the raw binary\n",
    "        n_channels:  number of channels in the raw binary recording \n",
    "        n_times:  length of waveform \n",
    "        spikes: 1D array containing spike times in sample rate of raw data\n",
    "        channels: load specific channels only\n",
    "        data_type: float32 for standardized data\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # ***** LOAD RAW RECORDING *****\n",
    "    if channels is None:\n",
    "        wfs = np.zeros((spikes.shape[0], n_times, n_channels), data_type)\n",
    "    else:\n",
    "        wfs = np.zeros((spikes.shape[0], n_times, channels.shape[0]), data_type)\n",
    "    if data_type =='float32':\n",
    "        data_len = 4\n",
    "    else:\n",
    "        data_len = 2\n",
    "\n",
    "    with open(filename, \"rb\") as fin:\n",
    "        for ctr,s in enumerate(spikes):\n",
    "            #print (ctr,s)\n",
    "            # index into binary file: time steps * 4  4byte floats * n_channels\n",
    "            fin.seek(s * data_len * n_channels, os.SEEK_SET)\n",
    "            wfs[ctr] = np.fromfile(\n",
    "                fin,\n",
    "                dtype=data_type,\n",
    "                count=(n_times * n_channels)).reshape(n_times, n_channels)[:,channels]\n",
    "    fin.close()\n",
    "    return wfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "templates:  (2413, 101, 512)\n",
      "spike_train:  (94794370, 2)\n",
      "geom:  (512, 2)\n",
      "(3072000000,)\n",
      "(6000000, 512)\n"
     ]
    }
   ],
   "source": [
    "# YASS DATASETS\n",
    "# load raw data and spike train\n",
    "root_dir = '/media/cat/12TB/insync_cm3746/Data/patrick/'\n",
    "\n",
    "temps = np.load('/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/liam/512channels/2009-04-13-5_120mins/tmp/output/templates/templates_0sec.npy')\n",
    "print (\"templates: \", temps.shape)\n",
    "\n",
    "spike_train = np.load('/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/liam/512channels/2009-04-13-5_120mins/tmp/output/spike_train.npy')\n",
    "print (\"spike_train: \", spike_train.shape)\n",
    "\n",
    "geom = np.loadtxt('/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/liam/512channels/2009-04-13-5_120mins/geom.txt')\n",
    "print (\"geom: \", geom.shape)\n",
    "\n",
    "data = np.fromfile('/mnt/adfe6e7b-b77b-4731-bc9e-e639667faba4/liam/512channels/2009-04-13-5_120mins/tmp/preprocess/standardized.bin','float32')\n",
    "print (data.shape)\n",
    "\n",
    "n_chan = 512\n",
    "data2D = data.reshape(-1,n_chan)\n",
    "print (data2D.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert 7 channel templates + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# chans enighour:  (7,)\n",
      "# chans enighour:  (7,)\n",
      " templates saved:  (29, 75, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cat/anaconda3/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# chans enighour:  (7,)\n",
      "# chans enighour:  (7,)\n",
      "# chans enighour:  (7,)\n",
      "# chans enighour:  (7,)\n",
      "# chans enighour:  (7,)\n",
      "# chans enighour:  (5,)\n",
      "# chans enighour:  (7,)\n",
      " templates saved:  (22, 75, 7)\n",
      "# chans enighour:  (7,)\n"
     ]
    }
   ],
   "source": [
    "# SAVE NEURONS ON MAX CHAN AND IMMEDIATE NEGIHBROU CHANS\n",
    "ptps = temps.ptp(1).max(1)\n",
    "max_chans = temps.ptp(1).argmax(1)\n",
    "\n",
    "root_dir = '/media/cat/12TB/insync_cm3746/Data/patrick/'\n",
    "\n",
    "ctr=0\n",
    "for chan in range(n_chan)[:10]:\n",
    "    idx2 = np.where(max_chans==chan)[0]\n",
    "    ptps_local = ptps[idx2]\n",
    "\n",
    "    locs = geom[chan]\n",
    "    \n",
    "    thresh=70\n",
    "    \n",
    "    chans_neighbour = []\n",
    "    for k in range(n_chan):\n",
    "        dist = np.linalg.norm(locs-geom[k])\n",
    "        if dist < thresh:\n",
    "            chans_neighbour.append(k)\n",
    "\n",
    "            # all chans within neighbourhood\n",
    "    chans_neighbour = np.unique(chans_neighbour)\n",
    "    \n",
    "    print (\"# chans enighour: \", chans_neighbour.shape)\n",
    "    #continue\n",
    "    #idx3 = np.where(ptps_local>8.0)[0]\n",
    "    \n",
    "    \n",
    "    if idx2.shape[0]>5:\n",
    "        \n",
    "        # save geometry of the data\n",
    "        np.savetxt(root_dir +'/chan_'+str(ctr)+'_geom.txt',\n",
    "               geom[chans_neighbour])\n",
    "        \n",
    "        \n",
    "        # save this channel and the data\n",
    "        np.save(root_dir+'/chan_'+str(ctr)+'_data_7chan.npy',\n",
    "                data2D[:,chans_neighbour])\n",
    "        \n",
    "        # save templates\n",
    "        temps_out=[]\n",
    "        for ch in chans_neighbour:\n",
    "            idx = np.where(max_chans==ch)[0]\n",
    "            #print (temps.shape, temps[idx].shape, temps[idx][:,:,chans_neighbour].shape)\n",
    "            temps_out.extend(temps[idx][:,:,chans_neighbour])  #Make sure you save max channel tempalte NOT \n",
    "            \n",
    "        temps_out = np.array(temps_out)[:,20:95]\n",
    "        print (\" templates saved: \", temps_out.shape)\n",
    "            \n",
    "        np.save(root_dir+'/chan_'+str(ctr)+'_templates.npy',\n",
    "                temps_out)\n",
    "        \n",
    "        # save spiketrains\n",
    "        times_out= []\n",
    "        for ch in chans_neighbour:\n",
    "            #idx = np.where\n",
    "            ids = np.where(max_chans==ch)[0]\n",
    "            for id_ in ids:\n",
    "                idx4 = np.where(spike_train[:,1]==id_)[0]\n",
    "                times = spike_train[idx4,0]\n",
    "                idx5 = np.where(times<(5*60*20000))[0]\n",
    "                times_out.append(times[idx5])\n",
    "        \n",
    "        np.save(root_dir+'/chan_'+str(ctr)+'_times.npy',\n",
    "                times_out)\n",
    "        #break    \n",
    "        ctr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all clustered spikes on a single channel:\n",
    "channel = 10\n",
    "\n",
    "ptps = temps.ptp(1).max(1)\n",
    "max_chans = temps.ptp(1).argmax(1)\n",
    "\n",
    "root_dir = '/media/cat/12TB/insync_cm3746/Data/patrick/'\n",
    "\n",
    "idx2 = np.where(max_chans==chan)[0]\n",
    "ptps_local = ptps[idx2]\n",
    "\n",
    "locs = geom[chan]\n",
    "\n",
    "thresh=70\n",
    "\n",
    "chans_neighbour = []\n",
    "for k in range(n_chan):\n",
    "    dist = np.linalg.norm(locs-geom[k])\n",
    "    if dist < thresh:\n",
    "        chans_neighbour.append(k)\n",
    "\n",
    "        # all chans within neighbourhood\n",
    "chans_neighbour = np.unique(chans_neighbour)\n",
    "\n",
    "print (\"# chans enighour: \", chans_neighbour.shape)\n",
    "#continue\n",
    "#idx3 = np.where(ptps_local>8.0)[0]\n",
    "\n",
    "\n",
    "if idx2.shape[0]>5:\n",
    "\n",
    "    # save geometry of the data\n",
    "    np.savetxt(root_dir +'/chan_'+str(ctr)+'_geom.txt',\n",
    "           geom[chans_neighbour])\n",
    "\n",
    "\n",
    "    # save this channel and the data\n",
    "    np.save(root_dir+'/chan_'+str(ctr)+'_data_7chan.npy',\n",
    "            data2D[:,chans_neighbour])\n",
    "\n",
    "    # save templates\n",
    "    temps_out=[]\n",
    "    for ch in chans_neighbour:\n",
    "        idx = np.where(max_chans==ch)[0]\n",
    "        #print (temps.shape, temps[idx].shape, temps[idx][:,:,chans_neighbour].shape)\n",
    "        temps_out.extend(temps[idx][:,:,chans_neighbour])  #Make sure you save max channel tempalte NOT \n",
    "\n",
    "    temps_out = np.array(temps_out)[:,20:95]\n",
    "    print (\" templates saved: \", temps_out.shape)\n",
    "\n",
    "    np.save(root_dir+'/chan_'+str(ctr)+'_templates.npy',\n",
    "            temps_out)\n",
    "\n",
    "    # save spiketrains\n",
    "    times_out= []\n",
    "    for ch in chans_neighbour:\n",
    "        #idx = np.where\n",
    "        ids = np.where(max_chans==ch)[0]\n",
    "        for id_ in ids:\n",
    "            idx4 = np.where(spike_train[:,1]==id_)[0]\n",
    "            times = spike_train[idx4,0]\n",
    "            idx5 = np.where(times<(5*60*20000))[0]\n",
    "            times_out.append(times[idx5])\n",
    "\n",
    "    np.save(root_dir+'/chan_'+str(ctr)+'_times.npy',\n",
    "            times_out)\n",
    "    #break    \n",
    "    ctr+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all clustered spikes on a single channel:\n",
    "channel = 10\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
