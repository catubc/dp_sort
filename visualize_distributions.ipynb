{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib tk\n",
    "%autosave 180\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from statsmodels import robust\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# run pca and visualize\n",
    "\n",
    "import scipy\n",
    "\n",
    "colors = [\n",
    "'black','blue','red','green','cyan','magenta','brown','pink',\n",
    "'orange','firebrick','lawngreen','dodgerblue','crimson','orchid','slateblue',\n",
    "'darkgreen','darkorange','indianred','darkviolet','deepskyblue','greenyellow',\n",
    "'peru','cadetblue','forestgreen','slategrey','lightsteelblue','rebeccapurple',\n",
    "'darkmagenta','yellow','hotpink']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize recomputed templates over time;\n",
    "def binary_reader_waveforms(filename, n_channels, n_times, spikes, channels=None, data_type='float32'):\n",
    "    ''' Reader for loading raw binaries\n",
    "    \n",
    "        standardized_filename:  name of file contianing the raw binary\n",
    "        n_channels:  number of channels in the raw binary recording \n",
    "        n_times:  length of waveform \n",
    "        spikes: 1D array containing spike times in sample rate of raw data\n",
    "        channels: load specific channels only\n",
    "        data_type: float32 for standardized data\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # ***** LOAD RAW RECORDING *****\n",
    "    if channels is None:\n",
    "        wfs = np.zeros((spikes.shape[0], n_times, n_channels), data_type)\n",
    "    else:\n",
    "        wfs = np.zeros((spikes.shape[0], n_times, channels.shape[0]), data_type)\n",
    "        \n",
    "    if data_type =='float32':\n",
    "        data_len = 4\n",
    "    else:\n",
    "        data_len = 2\n",
    "\n",
    "    with open(filename, \"rb\") as fin:\n",
    "        for ctr,s in enumerate(spikes):\n",
    "            #print (ctr,s)\n",
    "            # index into binary file: time steps * 4  4byte floats * n_channels\n",
    "            fin.seek(s * data_len * n_channels, os.SEEK_SET)\n",
    "            temp = np.fromfile(\n",
    "                fin,\n",
    "                dtype=data_type,\n",
    "                count=(n_times * n_channels))\n",
    "            wfs[ctr] = temp.reshape(n_times, n_channels)\n",
    "    fin.close()\n",
    "    return wfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "templates [n_neurons, n_times, n_channels]:  (2413, 101, 512)\n",
      "spike_train [times, neuron_ids]:  (94794370, 2)\n",
      "geom:  (512, 2)\n"
     ]
    }
   ],
   "source": [
    "# RETINAL DATASETS\n",
    "# load raw data and spike train\n",
    "root_dir = '/media/cat/12TB/insync_cm3746/Data/patrick/'\n",
    "\n",
    "# post spike sorting\n",
    "temps = np.load('/media/cat/4TBSSD/data/retina/2009/templates_0sec.npy')\n",
    "print (\"templates [n_neurons, n_times, n_channels]: \", temps.shape)\n",
    "\n",
    "spike_train = np.load('/media/cat/4TBSSD/data/retina/2009/spike_train.npy')\n",
    "print (\"spike_train [times, neuron_ids]: \", spike_train.shape)\n",
    "\n",
    "# raw data\n",
    "geom = np.loadtxt('/media/cat/4TBSSD/data/retina/2009/geom.txt')\n",
    "print (\"geom: \", geom.shape)\n",
    "\n",
    "n_chan = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28814662, 2)\n",
      "geom:  (384, 2)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL TAKES UP A LOT OF MEMORY\n",
    "if False:\n",
    "    data = np.fromfile('/media/cat/4TBSSD/data/retina/2009/standardized.bin','float32')\n",
    "    data2D = data.reshape(-1,n_chan)\n",
    "    print (\"Raw data (2D) shape: \", data2D.shape)\n",
    "\n",
    "    # visualize raw data; Channels x Time\n",
    "    sampling_rate = 20000.\n",
    "    t=np.arange(10000)/sampling_rate\n",
    "    for k in range(n_chan):\n",
    "        plt.plot(t,data2D[:10000,k]+k*50,c='black')\n",
    "    plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit:  0  , max chan:  251\n"
     ]
    }
   ],
   "source": [
    "# visualize templates; x-y geometry by n_times\n",
    "temps = np.load('/media/cat/4TBSSD/data/synthetic/run30_monotonic_3chans/ground_truth/templates_ground_truth.npy')\n",
    "max_chans = temps.ptp(1).argmax(1)\n",
    "unit = 0\n",
    "max_chan = max_chans[unit]\n",
    "print (\"Unit: \", unit, ' , max chan: ', max_chan)\n",
    "plt.plot((geom[:,0][:,None]+np.arange(101)/2.).T, \n",
    "         temps[unit] +\n",
    "         geom[:,1][None]/3.,c='black')\n",
    "plt.text(geom[max_chan,0],geom[max_chan,1]/3.,str(max_chan),fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at multiple templates for a specified channel\n",
    "max_chans = temps.ptp(1).argmax(1)\n",
    "max_chan = 10\n",
    "\n",
    "idx = np.where(max_chans==max_chan)[0]\n",
    "ctr=0\n",
    "for id_ in idx:\n",
    "    plt.plot((geom[:,0][:,None]+np.arange(101)/2.).T, \n",
    "         temps[id_] +\n",
    "         geom[:,1][None]/3.,c=colors[ctr])\n",
    "    ctr+=1\n",
    "plt.text(geom[max_chan,0],geom[max_chan,1]/3.,str(max_chan),fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron:  1393 waveform shape [n_spikes, n_times, n_chans]:  (2315, 101, 512)\n",
      "Neuron:  1502 waveform shape [n_spikes, n_times, n_chans]:  (1056, 101, 512)\n",
      "Neuron:  1536 waveform shape [n_spikes, n_times, n_chans]:  (177, 101, 512)\n"
     ]
    }
   ],
   "source": [
    "# grab all raw waveveforms for spikes on a single channel:\n",
    "channel = 10\n",
    "\n",
    "ptps = temps.ptp(1).max(1)\n",
    "max_chans = temps.ptp(1).argmax(1)\n",
    "\n",
    "root_dir = '/media/cat/12TB/insync_cm3746/Data/patrick/'\n",
    "filename = '/media/cat/4TBSSD/data/retina/2009/standardized.bin'\n",
    "n_channels = 512\n",
    "n_times = 101\n",
    "\n",
    "idx2 = np.where(max_chans==channel)[0]\n",
    "wfs_array = []\n",
    "for id_ in idx2:\n",
    "    \n",
    "    idx_3 = np.where(spike_train[:,1]==id_)[0]\n",
    "    spikes = spike_train[idx_3,0]\n",
    "    idx4 = np.where(spikes<(300*20000-100))[0]\n",
    "    spikes = spikes[idx4]-30\n",
    "   \n",
    "    wfs = binary_reader_waveforms(filename, n_channels, n_times, spikes, channels=None, data_type='float32')\n",
    "    print (\"Neuron: \", id_, \"waveform shape [n_spikes, n_times, n_chans]: \", wfs.shape)\n",
    "    wfs_array.append(wfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot multiple spikes from mulitple neurons\n",
    "units = [0,1,2]\n",
    "for unit in units:\n",
    "#     plt.plot((geom[:,0][:,None]+np.arange(101)/2.).T, \n",
    "#              temps[idx2[unit]] +\n",
    "#              geom[:,1][None]/3.,linewidth=5, c=colors[unit])\n",
    "    \n",
    "    for k in range(10):\n",
    "        plt.plot((geom[:,0][:,None]+np.arange(101)/2.).T, \n",
    "             wfs_array[unit][k] +\n",
    "             geom[:,1][None]/3.,c=colors[unit],alpha=.4)\n",
    "    \n",
    "plt.text(geom[max_chan,0],geom[max_chan,1]/3.,str(max_chan),fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA ON ALL CHANNELS\n",
      "Stacked matrix of all spikes for all neurons on particualr channel:  (3548, 101, 512)\n",
      "Flattened version for PCA:  (3548, 51712)\n",
      "post pca result:  (3548, 2)\n",
      "\n",
      "PCA ON NEIGHBOUR CHANNELS\n",
      "max chan:  10 has chans_neighbour:  [2, 5, 6, 10, 13, 14, 18]\n",
      "Flattened matrix of all spikes using neighbour chans only: (3548, 707)\n",
      "post pca result (3548, 2)\n",
      "\n",
      "PCA ON MAX CHANNEL\n",
      "Flattened matrix of all spikes using max chan only: (3548, 101)\n",
      "post pca result (3548, 2)\n"
     ]
    }
   ],
   "source": [
    "# VISUALIZE SORTED SPIKES using PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# compute PCS using all channels\n",
    "print (\"PCA ON ALL CHANNELS\")\n",
    "all_data = np.vstack(wfs_array)\n",
    "print (\"Stacked matrix of all spikes for all neurons on particualr channel: \", all_data.shape)\n",
    "all_data1D = all_data.reshape(all_data.shape[0],-1)\n",
    "print (\"Flattened version for PCA: \", all_data1D.shape)\n",
    "\n",
    "pcs_all_chans = pca.fit_transform(all_data1D) \n",
    "print (\"post pca result: \", pcs_all_chans.shape)\n",
    "print (\"\")\n",
    "\n",
    "# compute PCS on neighbour chans within 70 microns (Or can change this parameter)\n",
    "print (\"PCA ON NEIGHBOUR CHANNELS\")\n",
    "pca = PCA(n_components=2)\n",
    "locs = geom[channel]\n",
    "chans_neighbour = []\n",
    "thresh = 70 # distance in microns for channels to consider\n",
    "for k in range(geom.shape[0]):\n",
    "    dist = np.linalg.norm(locs-geom[k])\n",
    "    if dist < thresh:\n",
    "        chans_neighbour.append(k)\n",
    "print (\"max chan: \", channel, \"has chans_neighbour: \", chans_neighbour)\n",
    "\n",
    "all_data1D_neighbour_chans = all_data[:,:,chans_neighbour].reshape(all_data.shape[0],-1)\n",
    "print (\"Flattened matrix of all spikes using neighbour chans only:\", all_data1D_neighbour_chans.shape)\n",
    "\n",
    "pca_neighbour_chans = pca.fit_transform(all_data1D_neighbour_chans) \n",
    "print (\"post pca result\", pca_neighbour_chans.shape)\n",
    "print (\"\")\n",
    "\n",
    "# compute PCS on neighbour chans within 70 microns (Or can change this parameter)\n",
    "pca = PCA(n_components=2)\n",
    "print (\"PCA ON MAX CHANNEL\")\n",
    "all_data1D_max_chan = all_data[:,:,channel].reshape(all_data.shape[0],-1)\n",
    "print (\"Flattened matrix of all spikes using max chan only:\", all_data1D_max_chan.shape)\n",
    "\n",
    "pca_max_chan = pca.fit_transform(all_data1D_max_chan) \n",
    "#pca_max_chan = pca_local.fit_transform(all_data1D_max_chan)\n",
    "print (\"post pca result\", pca_max_chan.shape)\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PCA STUFF\n",
    "fig=plt.figure()\n",
    "plt.suptitle(\"PCA PLOTS ON CLUSTERED DATA\")\n",
    "ax = plt.subplot(2,2,1)\n",
    "plt.title(\"PCA on all channels\")\n",
    "ctr=0\n",
    "for k in range(len(wfs_array)):\n",
    "    x = pcs_all_chans[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "    y = pcs_all_chans[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "    plt.scatter(x,y,c=colors[k])\n",
    "    ctr+=wfs_array[k].shape[0]\n",
    "    \n",
    "ax = plt.subplot(2,2,2)\n",
    "plt.title(\"PCA on neighbour channels\")\n",
    "ctr=0\n",
    "for k in range(len(wfs_array)):\n",
    "    x = pca_neighbour_chans[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "    y = pca_neighbour_chans[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "    plt.scatter(x,y,c=colors[k])\n",
    "    ctr+=wfs_array[k].shape[0]\n",
    "\n",
    "ax = plt.subplot(2,2,3)\n",
    "plt.title(\"PCA on max channel\")\n",
    "ctr=0\n",
    "for k in range(len(wfs_array)):\n",
    "    x = pca_max_chan[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "    y = pca_max_chan[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "    plt.scatter(x,y,c=colors[k])\n",
    "    ctr+=wfs_array[k].shape[0]\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3548, 2)\n",
      "(3548, 2)\n",
      "(3548, 2)\n"
     ]
    }
   ],
   "source": [
    "# visualize data using UMAP\n",
    "import umap\n",
    "\n",
    "#for \n",
    "#embedding = umap.UMAP().fit_transform(digits.data)\n",
    "\n",
    "embedding_max_chan = umap.UMAP().fit_transform(all_data1D_max_chan) \n",
    "print (embedding_max_chan.shape)\n",
    "\n",
    "embedding_all_chan = umap.UMAP().fit_transform(all_data1D) \n",
    "print (embedding_all_chan.shape)\n",
    "\n",
    "embedding_neighbour_chan = umap.UMAP().fit_transform(all_data1D_neighbour_chans) \n",
    "print (embedding_neighbour_chan.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT UMAP PLOTS on CLUSTERED DATA\n",
    "fig=plt.figure()\n",
    "plt.suptitle(\"UMAP PLOTS on CLUSTERED DATA\")\n",
    "\n",
    "ax = plt.subplot(2,2,1)\n",
    "plt.title(\"UMAP on all channels\")\n",
    "ctr=0\n",
    "for k in range(len(wfs_array)):\n",
    "    x = embedding_all_chan[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "    y = embedding_all_chan[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "    plt.scatter(x,y,c=colors[k])\n",
    "    ctr+=wfs_array[k].shape[0]\n",
    "    \n",
    "ax = plt.subplot(2,2,2)\n",
    "plt.title(\"UMAP on neighbour channels\")\n",
    "ctr=0\n",
    "for k in range(len(wfs_array)):\n",
    "    x = embedding_neighbour_chan[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "    y = embedding_neighbour_chan[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "    plt.scatter(x,y,c=colors[k])\n",
    "    ctr+=wfs_array[k].shape[0]\n",
    "\n",
    "ax = plt.subplot(2,2,3)\n",
    "plt.title(\"UMAP on max channel\")\n",
    "ctr=0\n",
    "for k in range(len(wfs_array)):\n",
    "    x = embedding_max_chan[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "    y = embedding_max_chan[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "    plt.scatter(x,y,c=colors[k])\n",
    "    ctr+=wfs_array[k].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 SWITCH OVER THE DETECTED SPIKES NOW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10762737, 2)\n",
      "There were  19769  detected spikes\n",
      "Let's see what the missed spikes look like\n"
     ]
    }
   ],
   "source": [
    "# RETINAL DATASETS\n",
    "# do the same thing on all raw spikes detected by NN approach\n",
    "spike_train_detected = np.load('/media/cat/4TBSSD/data/retina/2009/spike_detections.npy')\n",
    "print (spike_train_detected.shape)\n",
    "\n",
    "channel=251\n",
    "idx = np.where(spike_train_detected==channel)[0]\n",
    "print (\"There were \", idx.shape[0], \" detected spikes\")\n",
    "spikes_detected_times = spike_train_detected[idx,0]\n",
    "\n",
    "print (\"Let's see what the missed spikes look like\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63041, 2)\n",
      "There were  294  in unit:  0\n",
      "Let's see what the missed spikes look like\n",
      "geom:  (384, 2)\n"
     ]
    }
   ],
   "source": [
    "# NEUROPIXEL DRIFT DATASETS\n",
    "# post spike sorting\n",
    "#temps = np.load('/media/cat/4TBSSD/data/retina/2009/templates_0sec.npy')\n",
    "#print (\"templates [n_neurons, n_times, n_channels]: \", temps.shape)\n",
    "\n",
    "#spike_train = np.load('/media/cat/4TBSSD/data/retina/2009/spike_train.npy')\n",
    "#print (\"spike_train [times, neuron_ids]: \", spike_train.shape)\n",
    "\n",
    "if False:\n",
    "    spike_train_detected = np.load('/media/cat/4TBSSD/data/synthetic/run30_monotonic_3chans/ground_truth/spike_train_ground_truth.npy')\n",
    "    print (spike_train_detected.shape)    \n",
    "    channel=252\n",
    "    idx = np.where(spike_train_detected[:,1]==channel)[0]\n",
    "    print (\"There were \", idx.shape[0], \" detected spikes on chan: \", channel)\n",
    "    spikes_detected_times = spike_train_detected[idx,0]\n",
    "else:\n",
    "    temps_gt = np.load('/media/cat/4TBSSD/data/synthetic/run31_monotonic_1.5chans_100units_0/ground_truth/templates_ground_truth.npy')\n",
    "    spike_train_gt = np.load('/media/cat/4TBSSD/data/synthetic/run31_monotonic_1.5chans_100units_0/ground_truth/spike_train_ground_truth.npy')\n",
    "    print (spike_train_gt.shape)\n",
    "   \n",
    "print (\"Let's see what the missed spikes look like\")\n",
    "\n",
    "# raw data\n",
    "geom = np.loadtxt('/media/cat/4TBSSD/data/synthetic/run26_random_walk/geom_384chans.txt')\n",
    "print (\"geom: \", geom.shape)\n",
    "\n",
    "n_channels = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# units is  (2,)   on channel:  181\n",
      "There were  573  spikes, in unit:  88\n",
      "There were  382  spikes, in unit:  93\n",
      "spikes:  (600,)\n",
      "...loading waveforms...\n",
      "waveform shape [n_spikes, n_times, n_chans]:  (600, 101, 384)\n"
     ]
    }
   ],
   "source": [
    "# grab all raw waveveforms for detected spikes\n",
    "\n",
    "root_dir = '/media/cat/12TB/insync_cm3746/Data/patrick/'\n",
    "filename = '/media/cat/4TBSSD/data/synthetic/run31_monotonic_1.5chans_100units_0/data_int16.bin'\n",
    "#n_channels = n_chans\n",
    "n_times = 101\n",
    "data_type = 'int16'\n",
    "\n",
    "# load a particular unit + all units on that channel:\n",
    "unit = 5\n",
    "max_chans=temps_gt.ptp(1).argmax(1)\n",
    "channel = max_chans[unit]\n",
    "\n",
    "channel = 181\n",
    "\n",
    "units = np.where(max_chans==channel)[0]\n",
    "print (\"# units is \", units.shape, \"  on channel: \", channel)\n",
    "\n",
    "spikes = []\n",
    "for unit in units:\n",
    "    idx = np.where(spike_train_gt[:,1]==unit)[0]\n",
    "    print (\"There were \", idx.shape[0], \" spikes, in unit: \", unit)\n",
    "    spikes.append(spike_train_gt[idx,0])\n",
    "    \n",
    "spikes = np.hstack(spikes)\n",
    "idx = np.argsort(spikes)\n",
    "spikes=spikes[idx]\n",
    "idx4 = np.where(np.logical_and(spikes>0, spikes<(300*20000-100)))[0] # clip so we don't go over 5 minutes\n",
    "spikes = spikes[idx4]  # shift the waveform a bit\n",
    "\n",
    "\n",
    "\n",
    "#spikes = spikes[::20]\n",
    "print (\"spikes: \", spikes.shape)\n",
    "\n",
    "print (\"...loading waveforms...\")\n",
    "wfs_detected = binary_reader_waveforms(filename, n_channels, n_times, spikes, channels=None, data_type=data_type)\n",
    "print (\"waveform shape [n_spikes, n_times, n_chans]: \", wfs_detected.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(wfs_detected[::10,:,251].T, c='black',alpha=.1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA ON ALL CHANNELS\n",
      "Stacked matrix of all spikes for all neurons on particualr channel:  (600, 101, 384)\n",
      "Flattened version for PCA:  (600, 38784)\n",
      "post pca result:  (600, 2)\n",
      "\n",
      "PCA ON NEIGHBOUR CHANNELS\n",
      "max chan:  181 has chans_neighbour:  [174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187]\n",
      "Flattened matrix of all spikes using neighbour chans only: (600, 1414)\n",
      "post pca result (600, 2)\n",
      "\n",
      "PCA ON MAX CHANNEL\n",
      "Flattened matrix of all spikes using max chan only: (600, 101)\n",
      "post pca result (600, 2)\n"
     ]
    }
   ],
   "source": [
    "# find max chan\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# compute PCS using all channels\n",
    "print (\"PCA ON ALL CHANNELS\")\n",
    "all_data_detected = wfs_detected\n",
    "#all_data_detected = wfs_\n",
    "print (\"Stacked matrix of all spikes for all neurons on particualr channel: \", all_data_detected.shape)\n",
    "all_data1D = all_data_detected.reshape(all_data_detected.shape[0],-1)\n",
    "print (\"Flattened version for PCA: \", all_data1D.shape)\n",
    "\n",
    "pcs_all_chans_detected = pca.fit_transform(all_data1D) \n",
    "print (\"post pca result: \", pcs_all_chans_detected.shape)\n",
    "print (\"\")\n",
    "\n",
    "# compute PCS on neighbour chans within 70 microns (Or can change this parameter)\n",
    "print (\"PCA ON NEIGHBOUR CHANNELS\")\n",
    "pca = PCA(n_components=2)\n",
    "locs = geom[channel]\n",
    "chans_neighbour = []\n",
    "thresh = 70 # distance in microns for channels to consider\n",
    "for k in range(geom.shape[0]):\n",
    "    dist = np.linalg.norm(locs-geom[k])\n",
    "    if dist < thresh:\n",
    "        chans_neighbour.append(k)\n",
    "print (\"max chan: \", channel, \"has chans_neighbour: \", chans_neighbour)\n",
    "\n",
    "all_data1D_neighbour_chans = all_data_detected[:,:,chans_neighbour].reshape(all_data_detected.shape[0],-1)\n",
    "print (\"Flattened matrix of all spikes using neighbour chans only:\", all_data1D_neighbour_chans.shape)\n",
    "\n",
    "pca_neighbour_chans = pca.fit_transform(all_data1D_neighbour_chans) \n",
    "print (\"post pca result\", pca_neighbour_chans.shape)\n",
    "print (\"\")\n",
    "\n",
    "# compute PCS on neighbour chans within 70 microns (Or can change this parameter)\n",
    "pca = PCA(n_components=2)\n",
    "print (\"PCA ON MAX CHANNEL\")\n",
    "all_data1D_max_chan = all_data_detected[:,:,channel].reshape(all_data_detected.shape[0],-1)\n",
    "print (\"Flattened matrix of all spikes using max chan only:\", all_data1D_max_chan.shape)\n",
    "\n",
    "#all_data1D_max_chan = pca.fit_transform(all_data1D_max_chan) \n",
    "pca_max_chan = pca.fit_transform(all_data1D_max_chan)\n",
    "print (\"post pca result\", pca_max_chan.shape)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PCA STUFF ON ALL DETETCED EVENTS\n",
    "fig=plt.figure()\n",
    "plt.suptitle(\"PCA PLOTS - ON ALL DETECTED EVENTS\")\n",
    "ax = plt.subplot(2,2,1)\n",
    "plt.title(\"PCA on all channels\")\n",
    "ctr=0\n",
    "#for k in range(len(wfs_array)):\n",
    "x = pcs_all_chans_detected[:,0]\n",
    "y = pcs_all_chans_detected[:,1]\n",
    "\n",
    "plt.scatter(x,y,c=np.arange(y.shape[0]),cmap='viridis',alpha=.2)\n",
    "    \n",
    "ax = plt.subplot(2,2,2)\n",
    "plt.title(\"PCA on neighbour channels\")\n",
    "x = pca_neighbour_chans[:, 0]\n",
    "y = pca_neighbour_chans[:, 1]\n",
    "plt.scatter(x,y,c=np.arange(y.shape[0]),cmap='viridis',alpha=.2)\n",
    "\n",
    "ax = plt.subplot(2,2,3)\n",
    "plt.title(\"PCA on max channel\")\n",
    "x = pca_max_chan[:,0]\n",
    "y = pca_max_chan[:,1]\n",
    "plt.scatter(x,y,c=np.arange(y.shape[0]),cmap='viridis',alpha=.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL Viauzlie PCs vs. time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE loading 1D arrays\n"
     ]
    }
   ],
   "source": [
    "# visualize data using UMAP on ALL DETECTED EVENTS\n",
    "all_data_detected = wfs_detected\n",
    "all_data1D = all_data_detected.reshape(all_data_detected.shape[0],-1)\n",
    "\n",
    "# compute PCS on neighbour chans within 70 microns (Or can change this parameter)\n",
    "locs = geom[channel]\n",
    "chans_neighbour = []\n",
    "thresh = 70 # distance in microns for channels to consider\n",
    "for k in range(geom.shape[0]):\n",
    "    dist = np.linalg.norm(locs-geom[k])\n",
    "    if dist < thresh:\n",
    "        chans_neighbour.append(k)\n",
    "\n",
    "all_data1D_neighbour_chans = all_data_detected[:,:,chans_neighbour].reshape(all_data_detected.shape[0],-1)\n",
    "\n",
    "# compute PCS on neighbour chans within 70 microns (Or can change this parameter)\n",
    "all_data1D_max_chan = all_data_detected[:,:,channel].reshape(all_data_detected.shape[0],-1)\n",
    "\n",
    "# computing embeddings for all chans x all xpikes\n",
    "# embedding_all_chan = umap.UMAP().fit_transform(all_data1D) \n",
    "# print (embedding_all_chan.shape)\n",
    "print (\"DONE loading 1D arrays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19769, 2)\n",
      "(19769, 2)\n"
     ]
    }
   ],
   "source": [
    "# compute embeddings for just some of the chans\n",
    "embedding_neighbour_chan = umap.UMAP().fit_transform(all_data1D_neighbour_chans) \n",
    "print (embedding_neighbour_chan.shape)\n",
    "\n",
    "embedding_max_chan = umap.UMAP().fit_transform(all_data1D_max_chan) \n",
    "print (embedding_max_chan.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT UMAP PLOTS on CLUSTERED DATA\n",
    "fig=plt.figure()\n",
    "plt.suptitle(\"UMAP PLOTS on DETECTED EVENTS\")\n",
    "\n",
    "ax = plt.subplot(2,2,1)\n",
    "plt.title(\"UMAP on all channels - OUT OF MEMORY ERROR\")\n",
    "ctr=0\n",
    "# for k in range(len(wfs_array)):\n",
    "#     x = embedding_all_chan[ctr:ctr+wfs_array[k].shape[0],0]\n",
    "#     y = embedding_all_chan[ctr:ctr+wfs_array[k].shape[0],1]\n",
    "#     plt.scatter(x,y,c=colors[k])\n",
    "#     ctr+=wfs_array[k].shape[0]\n",
    "    \n",
    "ax = plt.subplot(2,2,2)\n",
    "plt.title(\"UMAP on neighbour channels\")\n",
    "x = embedding_neighbour_chan[:,0]\n",
    "y = embedding_neighbour_chan[:,1]\n",
    "plt.scatter(x,y,c=colors[1],alpha=.1)\n",
    "\n",
    "ax = plt.subplot(2,2,3)\n",
    "plt.title(\"UMAP on max channel\")\n",
    "ctr=0\n",
    "x = embedding_max_chan[:,0]\n",
    "y = embedding_max_chan[:,1]\n",
    "plt.scatter(x,y,c=colors[2],alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
